{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d3d293b",
   "metadata": {},
   "source": [
    "# EDA and Feature Engineering and Selection Checklist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62944567",
   "metadata": {},
   "source": [
    "## Loading and basic details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6af41d2",
   "metadata": {},
   "source": [
    "1. Load the Dataset into DataFrame.\n",
    "    - If there are more than 1 data sources (CSVs, Tables, etc) for the same dataset, look for foreign keys to merge into 1 DataFrame.\n",
    "\n",
    "2. Take a look at the present columns using - df.columns\n",
    "\n",
    "3. For details like datatypes of each column use - df.info\n",
    "\n",
    "4. To get statistical details of numerical columns use - df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef377bf",
   "metadata": {},
   "source": [
    "## Data Analysis & Basic EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c08926",
   "metadata": {},
   "source": [
    "1. Find out Missing Values using,\n",
    "    - df.isnull()\n",
    "\n",
    "    - missing values percentage - df.isnull().mean()\n",
    "\n",
    "    - sns.heatmap(df.isnull(), yticklabels=False, cbar=False, cmap=\"viridis\")\n",
    "\n",
    "\n",
    "2. Duplicate values can be indentified using,\n",
    "\n",
    "    - df.duplicated().sum()\n",
    "\n",
    "\n",
    "3. Select columns based on their Datatypes\n",
    "\n",
    "    - df.select_dtypes(...)\n",
    "\n",
    "\n",
    "4. The Distribution of datapoints can be classified using\n",
    "\n",
    "    - df.feature_name.value_counts()\n",
    "\n",
    "\n",
    "5. This can be plotted in Pie Chart. Pie Charts are useful for finding the distribution of each feature's values.\n",
    "\n",
    "    - values = df.feature_1.value_counts().values\n",
    "\n",
    "    - index = df.feature_2.value_counts().index\n",
    "\n",
    "    - plt.pie(values, index, autopct='%1.2f%%')\n",
    "\n",
    "\n",
    "6. To Filter Datapoints, Group features like in SQL using,\n",
    "\n",
    "    - df.groupby([feat1, feat2, feat3]).size().reset_index()\n",
    "\n",
    "\n",
    "7. To get the Top 10 values of a Column\n",
    "\n",
    "    - df.groupby(['Feature_name']).size().reset_index().rename(columns={0:'Count'}).sort_values(by=\"Count\", ascending=False)[:10]\n",
    "\n",
    "\n",
    "8. To compare 2 features and distribution with another feature, draw a Bar Plot\n",
    "\n",
    "    - sns.barplot(x=\"feat1\", y=\"feat2\", data=df, hue='feat_3', palette=[...])\n",
    "\n",
    "\n",
    "9. For analysing each feature's count & distribution use a Count Plot.\n",
    "\n",
    "    - sns.countplot(x=\"feat1\", data=df, hue='feat2', palette=[...])\n",
    "\n",
    "\n",
    "10. Write Observations along the way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce7d9b3",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde4bda5",
   "metadata": {},
   "source": [
    "1. Type-convert **Numerical features** to int/float using .astype(int)\n",
    "\n",
    "    - **Binning** (for Tree models)\n",
    "        - splitting continuous values into Categories\n",
    "\n",
    "\n",
    "2. Encoding **Categorical features**\n",
    "\n",
    "    - For **Ordinal Features** that have a meaningful order use **Ordinal Encoding** (1, 2, 3...) like,\n",
    "        - Ratings - Poor, Fair, Good, Very Good\n",
    "        - Sizes - Small, Medium, Large, etc\n",
    "        - Use df.map({...}) to encode\n",
    "\n",
    "    - For **Nominal Features** that dont have any meaningful order use **Nominal Encoding** (one-hot, frequency, target, hashing, embeddings) methods like,\n",
    "        - Color - Red, Blue, Green\n",
    "        - Names - India, USA, UK\n",
    "        - Gender - Male, Female\n",
    "        - According to the number of Categories,\n",
    "            - less than < 20 categories   - **One-Hot Encoding**, use pd.get_dummies(df)\n",
    "            - from 20–100 categories - **Frequency/Count Encoding** or **Target Encoding**\n",
    "            - 100+ categories   - (especially with text/IDs)\t**Target Encoding, Hashing trick, or Embeddings**\n",
    "\n",
    "\n",
    "3. **Date/Time features**\n",
    "\n",
    "    - Split into,\n",
    "        - year (one_hot_enc)\n",
    "        - month (1,2,3 / one_hot_enc)\n",
    "        - day_of_week (one_hot_enc)\n",
    "        - hour\n",
    "        - season (summer, winter)\n",
    "        - is_weekend\n",
    "\n",
    "\n",
    "4. **Aggregating statistics** using one Feature\n",
    "\n",
    "    - **groupby(\"user_id\").agg([\"mean\", \"count\"])**\n",
    "    \n",
    "    - Gives mean and count for other Features of each unique user_id.\n",
    "    \n",
    "    - This adds new features describing user behavior.\n",
    "\n",
    "\n",
    "5. **Text Features**\n",
    "\n",
    "    - Sentiment score - positive/negative\n",
    "\n",
    "    - TF-IDF / Count Vector\n",
    "\n",
    "\n",
    "6. **Derived features** from Other Models\n",
    "\n",
    "    - **Clustering (Kmeans)** - assign cluster label as a feature.\n",
    "\n",
    "    - **PCA** - reduce dimensions into components.\n",
    "\n",
    "    - Example:\n",
    "        - Customer dataset → **KMeans clusters:**\n",
    "        - Customer A = cluster 1 (budget buyer)\n",
    "        - Customer B = cluster 2 (premium buyer)\n",
    "\n",
    "    - Now cluster_id is a feature in the main model.\n",
    "\n",
    "\n",
    "7. For categories with **High Cardinality**\n",
    "\n",
    "    - For Categories with many unique values, example : P12345, P54321, P11111 ... (millions of unique)\n",
    "\n",
    "        - **Hashing Trick**\n",
    "            - map categories into fixed-size buckets \n",
    "\n",
    "            - Hash into 1000 buckets: hash(P12345) % 1000 = 347\n",
    "\n",
    "        - **Embeddings**\n",
    "            - learn dense vector representations\n",
    "\n",
    "            - Train embeddings: P12345 → [0.2, -0.5, 0.7, ...]\n",
    "\n",
    "\n",
    "8. Handling Missing values\n",
    "\n",
    "    - (simple way)Fill it with the **Mode** of the Feature - df['feature'] = df[\"feature\"].fillna(df[\"feature\"].mode()[0])\n",
    "\n",
    "    - Or use other **Imputation** techniques of missing values using statistical methods like - **mean, median, interpolation, or ML.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837eeaee",
   "metadata": {},
   "source": [
    "## Imputation and Encoding"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4659ed2a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "4c86950a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "137dc9d4",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b833d33",
   "metadata": {},
   "source": [
    "1. Drop Constant Features\n",
    "\n",
    "    - drop the ones that are not \"Variant\" enough\n",
    "    - use VarianceThreshold with a desired threshold value and drop the constant_column"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8e342750",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "var_thres = VarianceThreshold(threshold=0)      # Removes Zero Variance features.\n",
    "var_thres.fit(X_train)\n",
    "constant_columns = [column for column in X_train.columns if column not in X_train.columns[var_thres.get_support()]]\n",
    "X_train.drop(constant_columns, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22b4ebf",
   "metadata": {},
   "source": [
    "2. From Correlation Matrix\n",
    "\n",
    "    - Correlation: “Does one feature increase or decrease in a linear relationship with another feature?”\n",
    "\n",
    "    - Compute the Pearson Correlation Matrix using the training set only.\n",
    "\n",
    "    - Identify highly correlated feature pairs (absolute correlation coefficient |r| > \"X\")\n",
    "\n",
    "    - From each highly correlated pair, choose one feature to drop (to reduce multicollinearity).\n",
    "\n",
    "    - Drop the selected features from both the training and testing sets to ensure consistency."
   ]
  },
  {
   "cell_type": "raw",
   "id": "03900b5b",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "cor = X_train.corr()\n",
    "sns.heatmap(cor, annot=True, cmap=plt.cm.CMRmap_r)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def highly_correlated_features(dataset, threshold):\n",
    "    col_corr = set()  # Set of all the names of correlated columns\n",
    "    corr_matrix = dataset.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold:     # The absolute value of the Coefficient\n",
    "                colname = corr_matrix.columns[i]  # Name of the column\n",
    "                col_corr.add(colname)\n",
    "    return col_corr\n",
    "\n",
    "corr_features = correlation(X_train, 0.7)       # Setting the Correlation Threshold to be 70%\n",
    "\n",
    "\n",
    "X_train.drop(corr_features, axis=1)\n",
    "X_test.drop(corr_features, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8af270",
   "metadata": {},
   "source": [
    "3. From **Mutual Information** for Classification\n",
    "\n",
    "    - Mutual Information: “Can this feature help predict or explain the target — in any way, linear or not?”\n",
    "\n",
    "    - It measures the dependency of the feature on the Target variable.\n",
    "\n",
    "    - Find MI values for each feature corresponding to the Target variable.\n",
    "\n",
    "    - Load those values into Series and sort it in Descending order.\n",
    "\n",
    "    - Select the Top \"X\" values."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b079e96c",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# ------- Observation -------\n",
    "mutual_info = mutual_info_classif(X_train, y_train)\n",
    "\n",
    "mutual_info = pd.Series(mutual_info)\n",
    "mutual_info.index = X_train.columns\n",
    "mutual_info.sort_values(ascending=False)\n",
    "\n",
    "mutual_info.sort_values(ascending=False).plot.bar(figsize=(20, 8))\n",
    "\n",
    "\n",
    "# ------- Implementation -------\n",
    "sel_top_cols = SelectKBest(mutual_info_classif, k=5)\n",
    "sel_top_cols.fit(X_train, y_train)\n",
    "X_train.columns[sel_top_cols.get_support()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4fc3dd",
   "metadata": {},
   "source": [
    "4. From **Mutual Information** for Regression\n",
    "\n",
    "    - Mutual Information: “Can this feature help predict or explain the target — in any way, linear or not?”\n",
    "\n",
    "    - It measures the dependency of the feature on the Target variable.\n",
    "\n",
    "    - Find MI values for each feature corresponding to the Target variable.\n",
    "\n",
    "    - Load those values into Series and sort it in Descending order.\n",
    "\n",
    "    - Select the Top \"X\" % out of the total features."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce48ce7a",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# ------- Observation -------\n",
    "mutual_info = mutual_info_regression(X_train, y_train)\n",
    "\n",
    "mutual_info = pd.Series(mutual_info)\n",
    "mutual_info.index = X_train.columns\n",
    "mutual_info.sort_values(ascending=False)\n",
    "\n",
    "mutual_info.sort_values(ascending=False).plot.bar(figsize=(15,5))\n",
    "\n",
    "\n",
    "# ------- Implementation -------\n",
    "selected_top_columns = SelectPercentile(mutual_info_regression, percentile=20)      # Selecting the Top 20 percentile\n",
    "selected_top_columns.fit(X_train, y_train)\n",
    "X_train.columns[selected_top_columns.get_support()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f96c922",
   "metadata": {},
   "source": [
    "## Imbalance Handling"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d9831ddf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "04677527",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
